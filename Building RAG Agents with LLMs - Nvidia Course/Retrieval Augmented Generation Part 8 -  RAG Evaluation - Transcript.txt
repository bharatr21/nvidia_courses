----------------------------------------------------------------------------------------------------
Retrieval Augmented Generation Part 8: RAG Evaluation
Video URL: https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-15-v1/RAG-Videos/RAG_08_Evaluation.mp4
----------------------------------------------------------------------------------------------------
 Hello and welcome back to the course.
 As we reach the final stretch, we're about to embark on the last section of the course covering Ragn agent evaluation.
 In this exercise, you'll get to create and run a starter evaluation chain to quantitatively evaluate your agent using LLM as a judge formulations.
 In this setting, you'll use a judge, LLM chain, to evaluate your model for its intended use case and features.
 The task of a judge can vary wildly depending on what you want to test for, but we will try formulation that asks the question, does my rag chain improve over a more naive approach? Specifically, we will generate a series of synthetic question answer pairs that combine details from randomly selected documents in your doc store.
 Then you will generate a rag response which tries to answer the question through its normal processes.
 We will then ask our judge LLM which one is better and impose our own assumptions or outputs specifications as necessary.
 If we restrict the output of our judge LLM, we'll be able to coerce the judge logic into a simple one for success and zero for failure.
 Then, repeating the experiment, we'll get an average percentage of success and can reasonably call this a metric.
 And that's it, a basic evaluator chain that tests a specific thing repeatedly.
 With a little bit of creativity, you can adopt the system to generate synthetic data based on your objective and can swap the formulation around to test different attributes or output different formats.
 Some common ones include co-sign similarity outputs or a list of extractions from which you can take the length.
 For inspiration, we highly recommend checking out the length chain evaluator chains and rag-as or rag assessment framework.
 These are great tools for inspiration but also should be customized and tuned as necessary to work with your needs and models in mind.
 Also as a next step for those interested, feel free to look into the use of evaluator agents.
 These are agent-like chains which do extra planning under the hood to perform multi-turn conversation evaluation or general trajectory evaluation.
 These are more advanced topics which will require quite a bit of LLM engineering to get working, but a good system will be able to perform more rigorous evaluations and can really help out to quantify the effective user experience in practice.
 So that's pretty much our last topic.
 It's actually really nice for us to finish off on evaluator chains for a few good reasons.
 For one thing, it's a really important tool to get right since actionable and easy to get information is always better when releasing a product out in the wild.
 It's also a great topic for an engineering standpoint because it's both not terribly difficult to implement but also opens up a lot of practice for the concepts we've been building up so far.
 And lastly, it gives us a really nice success metric which can be used to evaluate your final assessment.
 At the end of the notebook, you will find instructions on how to complete the course assessment.
 In this assessment, you will have to use the DocStore index that you accumulated in your previous notebook.
 You have to verify that at least one of those papers is a newer paper so modified within 30 days.
 And you'll have to take advantage of the Fast API Langserve routes systems established in notebook 3.
5.
 Using all these tools, you will be able to execute on the assessment by going into the Gradio application that is hosted on port 8090, clicking the Ragn pipeline, and then hitting Evaluate.
 I can do this right now if I would like but at the moment it's not actually going to do anything because I do not have the DocStore index directory inside of my environment.
 So this will, okay, complain about that exact issue.
 Fair enough.
 In order to implement your system, you will have to take advantage of, again, this notebook, the Langserve routes notebook.
 And there are some hints actually here about exactly what you need to do.
 The example shows that you can implement the basic chat feature earlier on in the course.
 And for this component, you'll need to implement some kind of a retriever and some kind of a generator component.
 Looking over at the actual service that you are interfacing with, again, frontend, server app.
py, you will notice that the notebook will direct you towards this secret line.
 So this is the line that you will want to invoke.
 And this will be part of, as you can see here, an assessment.
 So the Ragn evaluation chain will be running when you click this button.
 You will call Rag of L.
 And eventually, if your DocStore is good and you have your Ragn chain implemented and it all passes the assessment portion right here, the evaluation chain, then you will be able to get this message and you will be able to invoke the secret method to get your credit.
 Okay, that is the main idea for the assessment.
 Good luck and hope to see you in the next video for the wrap up.
